import os
import json
import logging
import time
from typing import List, Tuple

import dotenv
import requests
from openai import OpenAI
from astrapy import DataAPIClient

dotenv.load_dotenv()
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AstraOpenAIIngestNode:
    """
    This node ingests (stores) text items into an Astra DB collection
    with a vector embedding generated by an OpenAI model. The idea is 
    that you can then search these items using a separate search node.
    """

    @classmethod
    def INPUT_TYPES(cls):
        """
        Defines the input “sockets” for ComfyUI. 
        - item_text: The content you want to store
        - openai_api_key: Optionally provided by the user (though the code also loads from ENV)
        - astradb_token, astradb_endpoint, collection_name: For connecting to your Astra DB
        """
        return {
            "required": {
                "item_text": ("STRING", {"multiline": True, "default": "Enter your document text here"}),
                "astradb_token": ("STRING", {"multiline": False, "default": ""}),
                "astradb_endpoint": ("STRING", {"multiline": False, "default": ""}),
                "collection_name": ("STRING", {"multiline": False, "default": ""}),
                "chunk_size": ("INT", {"default": 1000}),
                "conversation_id": ("STRING", {"multiline": False, "default": ""}),
            }
        }

    RETURN_TYPES = ("STRING",)  # We'll return a string message indicating success or error
    FUNCTION = "ingest_to_astra"
    CATEGORY = "Astra / Ingest"

    def ingest_to_astra(
        self,
        item_text: str,
        astradb_token: str,
        astradb_endpoint: str,
        collection_name: str,
        chunk_size: int,
        conversation_id: str
    ) -> Tuple[str]:
        """
        Main function for ingestion:
          1) Generate embedding for item_text using OpenAI.
          2) Store the text & embedding in Astra DB as a new document.
        Returns a status message.
        """
        logger.info("Starting ingestion process...")
        
        chunks = self._chunk_text(item_text, chunk_size=chunk_size)
        logger.info(f"Splitting text into {len(chunks)} chunks.")
        
        if not chunks:
            return ("No text to ingest!",)

        embeddings = self._generate_openai_embedding(chunks)
        if isinstance(embeddings, str):
            return (f"Failed to generate embedding: {embeddings}",)
        
        try:
            inserted_count = self._store_in_astra_db(
                astradb_token,
                astradb_endpoint,
                collection_name,
                chunks,
                embeddings,
                conversation_id
            )
        except Exception as e:
            logger.exception("Error while storing document in Astra DB.")
            return (f"Error storing document: {e}",)

        return (f"Successfully inserted {inserted_count} documents.",)

    def _chunk_text(self, text: str, chunk_size: int = 1000) -> List[str]:
        """
        Splits `text` into a list of substrings, each with a maximum length
        of `chunk_size` characters.
        """
        text = text.strip()
        return [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]

    def _generate_openai_embedding(self, chunks: List[str], embedding_model: str = "text-embedding-3-small"):
        """
        Calls OpenAI to generate an embedding vector from the provided text. 
        Reads the API key from environment variables if no user input is given.
        """
        # If no openai_api_key is provided via the node input, fallback to .env
        final_api_key = os.environ.get("OPENAI_API_KEY")

        if not final_api_key:
            logger.error("OpenAI API key not set in node input or environment.")
            return "OpenAI API key not set"

        client = OpenAI(api_key=final_api_key)
        try:
            embedding_obj = client.embeddings.create(input=chunks, model=embedding_model)
            return [item.embedding for item in embedding_obj.data]
        except Exception as e:
            logger.exception("Error generating embedding from OpenAI.")
            return f"Error generating embedding: {e}"

    def _store_in_astra_db(
        self,
        astradb_token: str,
        astradb_endpoint: str,
        collection_name: str,
        chunks: List[str],
        embeddings: List[List[float]],
        conversation_id: str
    ):
        """
        Uses astrapy’s DataAPIClient to connect to Astra DB and insert a new document
        containing the user text and its associated embedding vector.
        """
        final_astra_token = astradb_token.strip() or os.environ.get("ASTRA_DB_APPLICATION_TOKEN")
        final_astra_endpoint = astradb_endpoint.strip() or os.environ.get("ASTRA_DB_API_ENDPOINT")

        if not final_astra_token or not final_astra_endpoint:
            raise ValueError("Missing Astra DB token or endpoint.")

        # Initialize the Astra DB client
        client = DataAPIClient(token=final_astra_token)
        db = client.get_database_by_api_endpoint(final_astra_endpoint)

        logger.info(f"Using collection: {collection_name} for ingestion.")
        collection = db.get_collection(collection_name)

        if len(chunks) != len(embeddings):
            raise ValueError("Mismatch between chunk count and embedding count.")
        
        documents = []
        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            documents.append(
                {
                    "content": chunk,
                    "conversation_id": conversation_id,
                    "timestamp": time.now().isoformat(),
                    "$vector": embedding,
                }
            )

        logger.info(f"Inserting {len(documents)} documents into Astra DB...")
        insertion_result = collection.insert_many(documents)
        logger.info(f"Inserted {len(insertion_result.inserted_ids)} items.")
        return len(insertion_result.inserted_ids)

